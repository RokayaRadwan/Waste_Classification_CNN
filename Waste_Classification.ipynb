{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84031e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105822f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for subfolder in ['default', 'real_world']:\n",
    "                subfolder_dir = os.path.join(class_dir, subfolder)\n",
    "                image_names = os.listdir(subfolder_dir)\n",
    "                random.shuffle(image_names)\n",
    "                \n",
    "                if split == 'train':\n",
    "                    image_names = image_names[:int(0.6 * len(image_names))]\n",
    "                elif split == 'val':\n",
    "                    image_names = image_names[int(0.6 * len(image_names)):int(0.8 * len(image_names))]\n",
    "                else:  # split == 'test'\n",
    "                    image_names = image_names[int(0.8 * len(image_names)):]\n",
    "                \n",
    "                for image_name in image_names:\n",
    "                    self.image_paths.append(os.path.join(subfolder_dir, image_name))\n",
    "                    self.labels.append(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ec3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child class inheretied from parent class (nn.Module: Base class for all neural network modules)\n",
    "class Custom_CNN_Model(nn.Module):\n",
    "    def __init__(self, waste_classes_nb):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extraction layers\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3,out_channels=32, kernel_size=3,stride=1,padding=1),\n",
    "                                   nn.BatchNorm2d(32))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32,out_channels=64, kernel_size=3,stride=1,padding=1),\n",
    "                                   nn.BatchNorm2d(64))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64,out_channels=128, kernel_size=3,stride=1,padding=1),\n",
    "                                   nn.BatchNorm2d(128))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128,out_channels=256, kernel_size=3,stride=1,padding=1),\n",
    "                                   nn.BatchNorm2d(256))\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(256,out_channels=512, kernel_size=3,stride=1,padding=1),\n",
    "                                   nn.BatchNorm2d(512))\n",
    "\n",
    "        # Activation function to introduce non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Reduce spatial dimension (downsampling)\n",
    "        self.maxpooling = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # Classifier layers\n",
    "        self.fc1 = nn.Linear(512*7*7, 512)     # each of 25088 inputs connects to all 512 neurons\n",
    "        self.fc2 = nn.Linear(512, waste_classes_nb)     # why???? (parameters?)\n",
    "\n",
    "        # drop/turn off 50% of nurons in the fc layer (p = 0.5)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, mod):\n",
    "        mod = self.conv1(mod)  # 32 x 224 x 224\n",
    "        mod = self.relu(mod)\n",
    "        mod = self.maxpooling(mod)  # 32 x 112 x 112\n",
    "        \n",
    "        mod = self.conv2(mod) # 64 x 112 x 112\n",
    "        mod = self.relu(mod)\n",
    "        mod = self.maxpooling(mod) # 64 x 56 x 56\n",
    "\n",
    "        mod = self.conv3(mod)  # 128 x 56 x 56\n",
    "        mod = self.relu(mod)\n",
    "        mod = self.maxpooling(mod) # 128 x 28 x 28\n",
    "\n",
    "        mod = self.conv4(mod)  # 256 x 28 x 28\n",
    "        mod = self.relu(mod)\n",
    "        mod = self.maxpooling(mod) # 256 x 14 x 14\n",
    "\n",
    "        mod = self.conv5(mod) # 512 x 14 x 14\n",
    "        mod = self.relu(mod)\n",
    "        mod = self.maxpooling(mod) # 512 x 7 x 7\n",
    "        \n",
    "        # Flatten the output of the final conv layer to a 1D vector for the FC layer\n",
    "        mod = mod.view(mod.size(0), -1)\n",
    "\n",
    "        mod = self.fc1(mod)\n",
    "        mod = self.drop(mod)\n",
    "        mod = self.relu(mod)\n",
    "        mod = self.fc2(mod)\n",
    "        return mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset path\n",
    "dataset_path = r'C:\\Users\\rouka\\OneDrive\\Desktop\\Uni\\year 3\\semester 2\\CSE 351 - Intro to AI\\Final_Project\\archive\\images\\images'\n",
    "\n",
    "# Set dataset hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001 # for optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a43f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and data loaders\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),     # randomly flip images\n",
    "    transforms.RandomRotation(10),         # small random rotation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = WasteDataset(dataset_path, split='train', transform=train_transform)\n",
    "val_dataset = WasteDataset(dataset_path, split='val', transform=val_test_transform)\n",
    "test_dataset = WasteDataset(dataset_path, split='test', transform=val_test_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2730167",
   "metadata": {},
   "outputs": [],
   "source": [
    "Waste_Classes = train_dataset.classes\n",
    "print(Waste_Classes)\n",
    "\n",
    "Waste_Classes_NB = len(Waste_Classes)\n",
    "print(Waste_Classes_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Custom_CNN_Model(Waste_Classes_NB).to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# reduces learning rate every 2 epochs by multiplying learning_rate by 0.5 every 2 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973aa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to apply early stopping if validation loss does not decrease\n",
    "# patience = max number or epochs without val_loss decrease\n",
    "best_val_loss = float(\"inf\")\n",
    "patience, counter = 5, 0\n",
    "\n",
    "# Lists to store the training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images = images.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # if val_loss decreased reset counter\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "\n",
    "    # else increase counter         \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:     #if counter exceedes patience stop training\n",
    "            print(\"Early stopping.\")\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
